{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KcEy_Kc-dcIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early environment setup\n"
      ],
      "metadata": {
        "id": "MemzGAPHdHAZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wWmbk_TOMItC"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n",
        "! pip install pycaret\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pycaret\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "lfl8WUmjNseN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive - applicable, if working on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ed_d8cegOEf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Fakenews/"
      ],
      "metadata": {
        "id": "p9x9flbJOXdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "true_data = pd.read_csv('/content/drive/MyDrive/dataset/True.csv')\n",
        "fake_data = pd.read_csv('/content/drive/MyDrive/dataset/Fake.csv')\n",
        "\n",
        "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
        "true_data['Target'] = ['True']*len(true_data)\n",
        "fake_data['Target'] = ['Fake']*len(fake_data)\n",
        "\n",
        "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
        "data = pd.concat([true_data, fake_data]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# See how the data looks like\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Oig1mA4SOepH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target column is made of string values True/Fake, let's change it to numbers 0/1 (Fake=1)\n",
        "data['label'] = (data['Target'] == 'Fake').astype(int)\n"
      ],
      "metadata": {
        "id": "_PTjqunJQo7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()\n"
      ],
      "metadata": {
        "id": "cbYt23P6QrBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')"
      ],
      "metadata": {
        "id": "w-MpuVwOSMd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Validation-Test set split into 80:10:10 ratio\n",
        "# Train-Temp split\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(data['title'], data['label'],\n",
        "                                                                    random_state=2018,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    stratify=data['Target'])\n",
        "# Validation-Test split\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                                                                random_state=2018,\n",
        "                                                                test_size=0.5,\n",
        "                                                                stratify=temp_labels)"
      ],
      "metadata": {
        "id": "5jHDnbfHVHmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o9kRRsiwdI42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data tokenize\n"
      ],
      "metadata": {
        "id": "NWOfjzE4dJsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT model and tokenizer via HuggingFace Transformers\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "Ykjvq4fUVkxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram of the number of words in train data 'title'\n",
        "seq_len = [len(title.split()) for title in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 40,color='firebrick')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Number of texts')"
      ],
      "metadata": {
        "id": "Q_tTAjmlV0Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT Tokeizer Functionality\n",
        "sample_data = [\"Build fake news model.\",\n",
        "               \"Using bert.\"]                                         # sample data\n",
        "tokenized_sample_data = tokenizer.batch_encode_plus(sample_data,\n",
        "                                                    padding=True)     # encode text\n",
        "print(tokenized_sample_data)\n"
      ],
      "metadata": {
        "id": "rLQmGSuGWeEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40 should cover most of the news\n",
        "MAX_LENGTH = 40\n",
        "\n"
      ],
      "metadata": {
        "id": "os0mzmuCXP7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_train = tokenizer(train_text.tolist(), max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokens_val   = tokenizer(val_text.tolist(),   max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "tokens_test  = tokenizer(test_text.tolist(),  max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "V9hgJsN9ZCj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loader structure definition\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32                                               #define a batch size\n",
        "\n",
        "train_data = TensorDataset(tokens_train[\"input_ids\"], tokens_train[\"attention_mask\"], torch.tensor(train_labels.tolist()))    # wrap tensors\n",
        "train_sampler = RandomSampler(train_data)                     # sampler for sampling the data during training\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "                                                              # dataLoader for train set\n",
        "val_data = TensorDataset(tokens_val[\"input_ids\"], tokens_val[\"attention_mask\"], torch.tensor(val_labels.tolist()))            # wrap tensors\n",
        "val_sampler = SequentialSampler(val_data)                     # sampler for sampling the data during training\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "                                                      # dataLoader for validation set"
      ],
      "metadata": {
        "id": "pMdprNlUZ-kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# partially frozen-BERT-based transfer learning (Freezing pre-trained layer approach)( 2 custom output layers added)"
      ],
      "metadata": {
        "id": "ntYWcvusdVkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qm2MWufwft7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze last 2 encoder layers\n",
        "for name, param in bert.encoder.layer.named_parameters():\n",
        "    layer_num = int(name.split('.')[0])\n",
        "    if layer_num >= 10:  # last 2 layers = layer 10 and 11\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Also unfreeze pooler if you're using pooler_output\n",
        "for param in bert.pooler.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "BxVvXdnPK33a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture (self defined Classifier)\n"
      ],
      "metadata": {
        "id": "cxzdfMF3ddPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5WjyvX6oKoKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert\n",
        "      self.dropout = nn.Dropout(0.1)            # dropout layer\n",
        "      self.relu =  nn.ReLU()                    # relu activation function\n",
        "      self.fc1 = nn.Linear(768,512)             # dense layer 1\n",
        "      self.fc2 = nn.Linear(512,2)               # dense layer 2 (Output layer)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)       # softmax activation function\n",
        "    def forward(self, sent_id, mask):           # define the forward pass\n",
        "      cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
        "                                                # pass the inputs to the model\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)                           # output layer\n",
        "      x = self.softmax(x)                       # apply softmax activation\n",
        "      return x\n",
        "\n",
        "model = BERT_Arch(bert)\n",
        "# Defining the hyperparameters (optimizer, weights of the classes and the epochs)\n",
        "# Define the optimizer\n",
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-5)          # learning rate\n",
        "# Define the loss function\n",
        "cross_entropy  = nn.NLLLoss()\n",
        "# Number of training epochs\n",
        "epochs = 3"
      ],
      "metadata": {
        "id": "wc_GqEQUfudp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training function\n",
        "\n",
        "[連結文字](https://)"
      ],
      "metadata": {
        "id": "bFFmhEw4jzN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining training and evaluation functions\n",
        "def train():\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  model.to(device)\n",
        "  for step,batch in enumerate(train_dataloader):                # iterate over batches\n",
        "    if step % 50 == 0 and not step == 0:                        # progress update after every 50 batches.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    sent_id, mask, labels = [x.to(device) for x in batch]\n",
        "    #batch = [r for r in batch]                                  # push the batch to gpu\n",
        "    #sent_id, mask, labels = batch\n",
        "    model.zero_grad()                                           # clear previously calculated gradients\n",
        "    preds = model(sent_id, mask)                                # get model predictions for current batch\n",
        "    loss = cross_entropy(preds, labels)                         # compute loss between actual & predicted values\n",
        "    total_loss = total_loss + loss.item()                       # add on to the total loss\n",
        "    loss.backward()                                             # backward pass to calculate the gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)     # clip gradients to 1.0. It helps in preventing exploding gradient problem\n",
        "    optimizer.step()                                            # update parameters\n",
        "    preds=preds.detach().cpu().numpy()                          # model predictions are stored on GPU. So, push it to CPU\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)                 # compute training loss of the epoch\n",
        "                                                                # reshape predictions in form of (# samples, # classes)\n",
        "  return avg_loss                                 # returns the loss and predictions\n",
        "\n",
        "def evaluate():\n",
        "  print(\"\\nEvaluating...\")\n",
        "  model.eval()                                    # Deactivate dropout layers\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  model.to(device)\n",
        "  for step,batch in enumerate(val_dataloader):    # Iterate over batches\n",
        "    if step % 50 == 0 and not step == 0:          # Progress update every 50 batches.\n",
        "                                                  # Calculate elapsed time in minutes.\n",
        "                                                  # Elapsed = format_time(time.time() - t0)\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "    sent_id, mask, labels = [x.to(device) for x in batch]\n",
        "                                                  # Report progress\n",
        "    # batch = [t for t in batch]                    # Push the batch to GPU\n",
        "    # sent_id, mask, labels = batch\n",
        "    with torch.no_grad():                         # Deactivate autograd\n",
        "      preds = model(sent_id, mask)                # Model predictions\n",
        "      loss = cross_entropy(preds,labels)          # Compute the validation loss between actual and predicted values\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "  avg_loss = total_loss / len(val_dataloader)         # compute the validation loss of the epoch\n",
        "  return avg_loss"
      ],
      "metadata": {
        "id": "eSW4XGOBj147"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and predict\n",
        "best_valid_loss = float('inf')\n",
        "train_losses=[]                   # empty lists to store training and validation loss of each epoch\n",
        "valid_losses=[]\n",
        "\n",
        "for epoch in range(3):\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    train_loss = train()                       # train model\n",
        "    valid_loss = evaluate()                    # evaluate model\n",
        "    if valid_loss < best_valid_loss:              # save the best model\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'fake_news_model.pt')\n",
        "    train_losses.append(train_loss)               # append training and validation loss\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "3Bu5z52wlSlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# performance evaluation\n"
      ],
      "metadata": {
        "id": "PjxpMvfmpZPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  preds = model(tokens_test['input_ids'].to(device), tokens_test['attention_mask'].to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(torch.tensor(test_labels.tolist()), preds))"
      ],
      "metadata": {
        "id": "iErgwDw6pqHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sampling\n"
      ],
      "metadata": {
        "id": "QAiymPgPrmNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unseen news headline\n",
        "\n",
        "unseen_news_text = [\n",
        "    # Real News\n",
        "    \"NASA's James Webb Space Telescope has discovered the most distant galaxy ever observed, dating back to just 300 million years after the Big Bang.\",\n",
        "    \"India successfully landed its Chandrayaan-3 mission on the Moon's south pole, becoming the first country to do so.\",\n",
        "    \"The European Union fined Meta €1.2 billion for violating GDPR by transferring user data to the U.S. without proper safeguards.\",\n",
        "    \"WHO officially declared COVID-19 is no longer a global public health emergency in May 2023.\",\n",
        "    \"Brazil's Amazon deforestation dropped by over 30% in the first half of 2023, following renewed environmental policies under President Lula.\",\n",
        "\n",
        "    # Fake / Ambiguous News\n",
        "    \"Scientists in Switzerland claim they have successfully transferred human consciousness into a USB stick during an AI experiment.\",\n",
        "    \"A startup in Silicon Valley is offering genetically modified wings to humans for $25,000, claiming it enables short-range flight.\",\n",
        "    \"Japan has announced it will build a second Mount Fuji using 3D printing to promote tourism and test volcano resilience.\",\n",
        "    \"A leaked UN document reveals that time travel was discovered in 1987 but has been kept classified by global powers.\",\n",
        "    \"Scientists discovered a new planet in our solar system composed entirely of cheese, visible only during lunar eclipses.\"\n",
        "]\n",
        "\n",
        "# Tokenize using modern API and return PyTorch tensors\n",
        "MAX_LENGTH = 15\n",
        "tokens_unseen = tokenizer(\n",
        "    unseen_news_text,\n",
        "    max_length=MAX_LENGTH,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Move inputs to device (e.g. GPU)\n",
        "unseen_seq = tokens_unseen[\"input_ids\"].to(device)\n",
        "unseen_mask = tokens_unseen[\"attention_mask\"].to(device)\n",
        "\n",
        "# Predict\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(unseen_seq, unseen_mask)\n",
        "    preds = torch.argmax(preds, dim=1).cpu().numpy()\n",
        "\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "1kOCpfiIq-zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saved trained model\n"
      ],
      "metadata": {
        "id": "w2avnP6ZtQe6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vD-0uyXStWUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IF CPU\n",
        "# model = BERT_Arch(bert)\n",
        "# model = model.to(\"cpu\")\n",
        "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/Fakenews_prj/fake_news_model.pt\", map_location=torch.device(\"cpu\")))\n",
        "# model.eval()\n",
        "\n",
        "\n",
        "model = BERT_Arch(bert)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Fakenews/fake_news_model.pt\"))\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "VomPfWkpHlAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F1lHE9WqIUng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
